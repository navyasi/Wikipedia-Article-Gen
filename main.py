import sys
import os
import asyncio
from dotenv import load_dotenv
from loguru import logger

# Add src to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from storm.core import StormSystem, StormConfig
from storm.models.llama_client import LlamaConfig
from storm.search.web_search import WebSearchManager, SerperSearchProvider

async def main():
    load_dotenv()
    logger.add("storm.log", rotation="10 MB")
    
    print("üå™Ô∏è  STORM Article Generator")
    print("=" * 50)
    
    # Get user choice
    print("Choose generation mode:")
    print("1. Outline only (faster)")
    print("2. Full article (slower, more comprehensive)")
    
    try:
        choice = input("Enter choice (1 or 2): ").strip()
    except KeyboardInterrupt:
        print("\nüëã Goodbye!")
        return
    
    topic = input("Enter topic to research: ").strip()
    if not topic:
        topic = "Artificial Intelligence"
    
    print(f"\nüéØ Researching: {topic}")
    print("üîÑ This may take a few minutes...")
    
    # Configuration
    llama_config = LlamaConfig(
        host=os.getenv("OLLAMA_HOST", "http://localhost:11434"),
        model=os.getenv("OLLAMA_MODEL", "llama3.2:latest")
    )
    
    storm_config = StormConfig(
        llama_config=llama_config,
        max_related_topics=4,
        max_perspectives=4,
        max_conversation_rounds=3,
        exclude_wikipedia=True
    )
    
    # Setup search
    serper_api_key = os.getenv("SERPER_API_KEY")
    if not serper_api_key:
        print("‚ùå Need SERPER_API_KEY in .env file")
        print("üí° Get free key from https://serper.dev")
        return
    
    search_provider = SerperSearchProvider(serper_api_key)
    search_manager = WebSearchManager(search_provider, ["en.wikipedia.org"])
    
    storm = StormSystem(storm_config, search_manager, topic)
    
    try:
        if choice == "2":
            # Full article generation
            print("üìù Generating full article...")
            outline, article, sources = await storm.generate_full_article(topic)
            
            print("\n" + "="*80)
            print(f"üìÑ FULL ARTICLE: {topic}")
            print("="*80)
            print(article)
            
            # Save to file
            filename = f"{topic.replace(' ', '_').replace('/', '_')}_article.md"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"# Research Report: {topic}\n\n")
                f.write(f"**Generated by STORM on:** {asyncio.get_event_loop().time()}\n\n")
                f.write("## Outline\n\n")
                f.write(outline)
                f.write("\n\n## Full Article\n\n")
                f.write(article)
                f.write(f"\n\n## Sources ({len(sources)})\n\n")
                for i, source in enumerate(sources, 1):
                    f.write(f"{i}. [{source.title}]({source.url})\n")
                    f.write(f"   {source.snippet[:200]}...\n\n")
            
            print(f"\nüíæ Full article saved to: {filename}")
            
        else:
            # Outline only (original behavior)
            print("üìã Generating outline...")
            outline, sources = await storm.generate_article_outline(topic)
            
            print("\n" + "="*60)
            print(f"üìã OUTLINE: {topic}")
            print("="*60)
            print(outline)
        
        print(f"\nüìö Sources found: {len(sources)}")
        print("\nTop sources:")
        for i, source in enumerate(sources[:5], 1):
            print(f"{i}. {source.title[:60]}...")
            print(f"   üîó {source.url}")
            
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Generation stopped by user")
    except Exception as e:
        print(f"‚ùå Error: {e}")
        logger.error(f"STORM error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
